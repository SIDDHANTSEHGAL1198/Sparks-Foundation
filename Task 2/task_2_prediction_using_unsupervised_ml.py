# -*- coding: utf-8 -*-
"""Task 2-Prediction using Unsupervised ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IPXU-C9vhxiAtMyZuj6Jg_48-eK2pNs0

# Problem Statement
### From the given ‘Iris’ dataset, predict the optimum number of clusters and represent it visually

### Unsupervised learning is commonly used for finding meaningful patterns and groupings inherent in data, extracting generative features, and exploratory purposes. Number of optimizers can be predicted by using 2 algorithms:
### 1) K-Means Algorithms
### 2) Hierarchical Algorithms

# Import libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go



"""# Importing Dataset"""

dataset=pd.read_csv('Iris.csv')
dataset

dataset.info()

dataset.describe()

dataset['Species'].unique()

"""### As we are predicting number of clusters we have to drop output column .

# Exploratory Data Analysis
"""

fig=px.scatter(dataset,x='SepalLengthCm',y='SepalWidthCm')
fig.show()

fig=px.scatter(dataset,x='PetalLengthCm',y='PetalWidthCm')
fig.show()



"""# Unsupervised ML"""

dataset=dataset.drop(['Id','Species'],axis=1)
dataset

x=dataset.iloc[:,:].values
x

"""# K-Means"""

wcss=[]
from sklearn.cluster import KMeans
for i in range(1,11):
  kmeans=KMeans(n_clusters=i,init='k-means++',random_state=177013)
  kmeans.fit(x)
  wcss.append(kmeans.inertia_)

"""# Finding Optimal Number of Clusters using Elbow Method"""

plt.plot(range(1,11),wcss)
plt.title('Finding optimal number of clusters using ELBOW METHOD')
plt.xlabel('Number of clusters')
plt.ylabel('Scores of wcss')
plt.show()

"""### Number of clusters determined by elbow method is 3."""

kmeans=KMeans(n_clusters=3,init='k-means++',random_state=177013)
pred=kmeans.fit_predict(x)     # Predicting to which cluster they belong based on flower properties
pred

plt.scatter(x[pred==0,0],x[pred==0,1],s=100,c='green',label='Cluster 1 - Iris-sentosa')
plt.scatter(x[pred==1,0],x[pred==1,1],s=100,c='blue',label='Cluster 2 - Iris-versicolor')
plt.scatter(x[pred==2,0],x[pred==2,1],s=100,c='red',label='Cluster 3 - Iris-verginica')
plt.title('K-Means Clustering')
plt.xlabel('Petal length')
plt.ylabel('Sepal length')
plt.legend()
plt.show()



"""# Hierarchical Clustering"""

import scipy.cluster.hierarchy as sch 
dendrogram =sch.dendrogram(sch.linkage(x,method='ward'))
plt.title("Finding Optimal number of clusters")
plt.ylabel("Eucledian Distance")
plt.xlabel("Flower Composition")
plt.show()

from sklearn.cluster import AgglomerativeClustering
hc=AgglomerativeClustering(n_clusters=3,affinity='euclidean',linkage='ward')
hc_pred=hc.fit_predict(x)
hc_pred

"""# Visualisation for Clusters"""

plt.scatter(x[hc_pred==0,0],x[hc_pred==0,1],s=100,c='red',label='Type 1')
plt.scatter(x[hc_pred==1,0],x[hc_pred==1,1],s=100,c='blue',label='Type 2')
plt.scatter(x[hc_pred==2,0],x[hc_pred==2,1],s=100,c='yellow',label='Type 3')

plt.title('Flower Classification')
plt.xlabel('Flower compostion')
plt.ylabel(' ')
plt.legend()
plt.show()

